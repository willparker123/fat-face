{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit (conda)"
  },
  "interpreter": {
   "hash": "145042b0df78178b62874135a1b9b6ca32ed29550eba7a95d430597b439938a0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Comparing Counterfactual Explanations methods\n",
    "\n",
    "This Jupyter notebook is for comparing the Contrastive Explanations Method [https://papers.nips.cc/paper/2018/file/c5ff2543b53f4cc0ad3819a36752467b-Paper.pdf] and the Feasible and Actionable Counterfactual Method (FACE - https://arxiv.org/abs/1909.09369) using K-Nearest Neighbour, Epsilon graphs (using CARLA repo) and Kernel Density Estimator (GraphCounterfactuals).\n",
    "\n",
    "They will be compared visually on the tabular California Housing Dataset and the MNIST dataset.\n",
    "\n",
    "This notebook requires python 3.7.8/9"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## MNIST Analysis\n",
    "\n",
    "\"To this end, we trained a Neural Network\n",
    "with two hidden layers of length 10 and ReLU activation functions.\n",
    "FACE was initialised with w(z) = −loд(z) as the weight function\n",
    "and the l2-norm as the distance function.\""
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, './repoCarla') # location of src \n",
    "\n",
    "from carla.data.api import Data\n",
    "from carla.models.catalog import MLModelCatalog\n",
    "from carla.models.api import MLModel\n",
    "from carla.recourse_methods.catalog.face import Face\n",
    "from carla import Data, MLModel\n",
    "from carla.recourse_methods import Face, CEM\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random as rand\n",
    "#import pymc3 as pm\n",
    "from typing import Union\n",
    "import math\n",
    "import time\n",
    "from sklearn.datasets import fetch_openml, fetch_california_housing\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import Normalizer, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from scipy.spatial.distance import cdist, pdist\n",
    "from scipy.stats import gaussian_kde\n",
    "import networkx as nx\n",
    "from sklearn.neighbors import DistanceMetric\n",
    "\n",
    "X_MNIST,y_MNIST = fetch_openml('mnist_784', return_X_y=True)\n",
    "X_train_MNIST, X_test_MNIST, y_train_MNIST, y_test_MNIST = train_test_split(\n",
    "    X_MNIST, y_MNIST, test_size=0.25, random_state=4)\n",
    "X_train_MNIST = X_train_MNIST/255.\n",
    "X_test_MNIST = X_test_MNIST/255.\n",
    "MNIST_full = fetch_openml('mnist_784')\n",
    "desc_MNIST = MNIST_full['DESCR']\n",
    "feature_names_MNIST = MNIST_full['feature_names']\n",
    "target_names_MNIST = MNIST_full['target_names']\n",
    "names_MNIST = target_names_MNIST.extend(feature_names_MNIST)\n",
    "print(\"NAMES: \")\n",
    "print(names_MNIST)\n",
    "dataMNIST = fetch_openml('mnist_784', return_X_y=False, as_frame=True)['frame']\n",
    "dataMNISTDict = {k: v for k, v in enumerate(dataMNIST)}\n",
    "# first implement the dataset wrapper\n",
    "class MNISTData(Data):\n",
    "    def __init__(self):\n",
    "        # the dataset could be loaded in the constructor\n",
    "        self._dataset = fetch_openml('mnist_784', return_X_y=False, as_frame=True)['frame']\n",
    "\n",
    "    def categoricals(self):\n",
    "        # this property contains a list of all categorical features (column string names)\n",
    "        return target_names_MNIST\n",
    "\n",
    "    def continous(self):\n",
    "        # this property contains a list of all continuous features (column string names)\n",
    "        return feature_names_MNIST\n",
    "\n",
    "    def immutables(self):\n",
    "        # this property contains a list of features which should not be changed by the recourse method (column string names)\n",
    "        return []\n",
    "\n",
    "    def target(self):\n",
    "        # this property contains the feature name of the target column (string)\n",
    "        return target_names_MNIST[0]\n",
    "\n",
    "    def raw(self):\n",
    "        # this property contains the not encoded and not normalized, raw dataset\n",
    "        return self._dataset\n",
    "\n",
    "# second, implement the black-box-model wrapper\n",
    "class ANNModel(MLModel):\n",
    "    def __init__(self, data):\n",
    "        # the constructor can be used to load or build an arbitrary black-box-model\n",
    "        self._mymodel = load_model(\"ann2\")\n",
    "\n",
    "        # this property contains a fitted scaler to normalize input data\n",
    "        # MinMaxScaler from sklearn is predefined, but can be redefined by every other sklearn scaler\n",
    "        self.scaler = Normalizer().fit(X_train_MNIST)\n",
    "\n",
    "        # the same is possible for data encoding\n",
    "        # OneHotEncoder from sklearn with dropped first column for binary data is predefined, but can be\n",
    "        # changed into any other sklearn encoder.\n",
    "        self.encoder = OneHotEncoder(handle_unknown='ignore').fit(y_train_MNIST.reshape(-1, 1))\n",
    "\n",
    "    def feature_input_order(self):\n",
    "        # this property contains a list of the correct input order of features for the ml model\n",
    "        return feature_names_MNIST\n",
    "\n",
    "    def backend(self):\n",
    "        # this property contains a string with the used backend of the model\n",
    "        return \"pytorch\"\n",
    "\n",
    "    def raw_model(self):\n",
    "        # this property contains the fitted/ loaded black-box-model\n",
    "        return self._mymodel\n",
    "\n",
    "    def predict(self, x: Union[np.ndarray, pd.DataFrame]):\n",
    "        # the predict function outputs the continous prediction of the model, similar to sklearn.\n",
    "        return self._mymodel.predict(x)\n",
    "\n",
    "    def predict_proba(self, x: Union[np.ndarray, pd.DataFrame]):\n",
    "        # the predict_proba method outputs the prediction as class probabilities, similar to sklearn\n",
    "        return self._mymodel.predict_proba(x)\n",
    "\n",
    "def load_model(modeltype):\n",
    "    if modeltype == \"ann2\":\n",
    "        MLPClassifier(alpha=1e-05, hidden_layer_sizes=(10, 10), random_state=1,\n",
    "              solver='adam')\n",
    "\n",
    "# after implementing the user-specific model and dataset, the call of the recourse method,\n",
    "# and the generation of counterfactuals stays the same.\n",
    "dataset = MNISTData()\n",
    "model = ANNModel(dataset)\n",
    "print(\"-----DATASET-----\")\n",
    "print(fetch_openml('mnist_784', return_X_y=False, as_frame=True)['frame'])\n",
    "print(\"-----------------\")\n",
    "# get some factuals from the data to generate counterfactual examples\n",
    "factuals = dataset.raw().iloc[:10]\n",
    "\n",
    "# summarize loaded dataset\n",
    "print('Train: X=%s, y=%s' % (X_train_MNIST.shape, y_train_MNIST.shape))\n",
    "print('Test: X=%s, y=%s' % (X_test_MNIST.shape, y_test_MNIST.shape))\n",
    "# plot first few images\n",
    "for i in range(9):\n",
    "\t# define subplot\n",
    "\tplt.subplot(330 + 1 + i)\n",
    "\t# plot raw pixel data\n",
    "\tplt.imshow(X_train_MNIST[i], cmap=pyplot.get_cmap('gray'))\n",
    "# show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_threshold = 0.5 # 1.2\n",
    "density_threshold = 0.008\n",
    "pred_threshold = 0.999 #  0.9\n",
    "counterfactuals_FACE_KDE = []\n",
    "randI = random.randrange(0, len(dataMNIST))\n",
    "example = dataMNIST.iloc[randI]\n",
    "example_label = model.predict(example)\n",
    "data_example, start_node = nearest(example, example_label)\n",
    "if dist_face(example, data_example) > dist_threshold:\n",
    "    print('Data point does not meet distance threshold')\n",
    "\n",
    "def dist_face(x,y):\n",
    "    return cdist(x.reshape(1,-1), y.reshape(1,-1), metric='euclidean')\n",
    "\n",
    "def kernel(x,y):\n",
    "    kernel = gaussian_kde(X.T)\n",
    "    return kernel((x.reshape(1,-1), y.reshape(1,-1)))\n",
    "print(dataMNIST['label'])\n",
    "print(data_same_label)\n",
    "# nearest neighbour\n",
    "dist = DistanceMetric.get_metric('euclidean')\n",
    "def nearest(example, label):\n",
    "    data_same_label = dataMNIST[dataMNIST['class']==int(label)]\n",
    "    distances = dist.pairwise(data_same_label[['x','y']], example)\n",
    "    data_same_label.insert(3, 'distances', distances, True)\n",
    "    index_min = data_same_label.distances.idxmin()\n",
    "    nearest_data = np.array([data_same_label.loc[index_min]['x'], data_same_label.loc[index_min]['y']]).reshape(1,-1)\n",
    "    point = data_same_label.loc[index_min].name\n",
    "    return nearest_data, index_min\n",
    "\n",
    "# create weights based on distance threshold\n",
    "kernel = gaussian_kde(X.T)\n",
    "\n",
    "m, n = X.shape\n",
    "w_ij = [0] * int(m*(m-1)*0.5)\n",
    "edge_weights = []\n",
    "k = 0\n",
    "q = 0\n",
    "for edge_from in range(m):\n",
    "    k = 1 + edge_from\n",
    "    for edge_to in range(k,m):\n",
    "        if dist_face(X[edge_from], X[edge_to])[0][0] < dist_threshold:\n",
    "            w_ij[q] = -np.log(kernel((X[edge_from] + X[edge_to]) /2)[0] * dist_face(X[edge_from], X[edge_to])[0][0])\n",
    "        edge_weights.append((edge_from, edge_to, {'weight':w_ij[q]}))\n",
    "        q += 1\n",
    "\n",
    "nonzero_edge = []\n",
    "for i in range(len(edge_weights)):\n",
    "    if edge_weights[i][2]['weight'] != 0:\n",
    "        nonzero_edge.append(edge_weights[i])\n",
    "\n",
    "        \n",
    "# create graph with nodes that meet density threshold\n",
    "G = nx.Graph()\n",
    "\n",
    "G.add_nodes_from(range(len(X)))\n",
    "G.add_edges_from(nonzero_edge)\n",
    "\n",
    "# low_density = []\n",
    "# for i in range(m):\n",
    "#     if kernel(X[i]) < density_threshold:\n",
    "#         low_density.append(i)\n",
    "# G.remove_nodes_from(low_density)\n",
    "\n",
    "print('nodes that meet density theshold: ', G.number_of_nodes())\n",
    "print('edges that meet distance theshold: ', G.number_of_edges())\n",
    "\n",
    "_, ax = plt.subplots(figsize=(12,12))\n",
    "c = \"br\"\n",
    "nx.draw_networkx(G, pos=X, node_color=[c[int(y)] for y in Y], ax=ax)\n",
    "\n",
    "target_label = 0 if example_label == 1 else 1\n",
    "target_data = data_df[data_df['label']==int(target_label)]\n",
    "target_nodes = list(set(list(G.nodes())).intersection(target_data.index))\n",
    "\n",
    "pred_threshold_fail = []\n",
    "for i in target_data.index:\n",
    "    index = target_data.loc[i]\n",
    "    #np.array([index['x'],index['y']]).reshape(1, -1)\n",
    "    prob_target = model.predict_proba(np.array(target_data[i]).reshape(1, -1))[0][target_label]\n",
    "    if prob_target < pred_threshold:\n",
    "        pred_threshold_fail.append(i)\n",
    "\n",
    "        \n",
    "length, path = nx.multi_source_dijkstra(G,target_nodes,target=start_node)\n",
    "path = path[::-1]\n",
    "\n",
    "while path[-1] in pred_threshold_fail:\n",
    "    index = [i for i,node in enumerate(target_nodes) if node == path[-1]][0]\n",
    "    del target_nodes[index]\n",
    "          \n",
    "    length, path = nx.multi_source_dijkstra(G,target_nodes,target=start_node)\n",
    "    path = path[::-1]\n",
    "# plot\n",
    "x_path = []\n",
    "y_path = []\n",
    "for node in path:\n",
    "    x_path.append(data_dict[node][0])\n",
    "    y_path.append(data_dict[node][1])\n",
    "counterfactuals_FACE_KDE.append(x_path[-1],y_path[0-1])\n",
    "\n",
    "\n",
    "\n",
    "#\"mode\": str ['knn', 'epsilon'],\n",
    "#                \"fraction\": float [0 < x < 1]}  determines fraction of data set to be used to\n",
    "#                                                construct neighbourhood graph\n",
    "# load recourse model with model specific hyperparameter\n",
    "rcm_FaceKnn = Face(model, {\"mode\": \"knn\", \"fraction\": 0.1})\n",
    "rcm_FaceE = Face(model, {\"mode\": \"epsilon\", \"fraction\": 0.1})\n",
    "# generate counterfactual examples\n",
    "counterfactuals_FACE_knn = rcm_FaceKnn.get_counterfactuals(factuals)\n",
    "counterfactuals_FACE_epsilon = rcm_FaceE.get_counterfactuals(factuals)\n",
    "print(\"-----COUNTERFACTUALS - FACE (KNN)-----\")\n",
    "print(counterfactuals_FACE_knn)\n",
    "print(\"--------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\r\n",
    "Initialisation for the Contrastive Explanation Method (CEM).\r\n",
    "Paper: https://arxiv.org/abs/1802.07623\r\n",
    "\r\n",
    "Restrictions\r\n",
    "------------\r\n",
    "- Works currently only on Tensorflow models\r\n",
    "\r\n",
    "Parameters\r\n",
    "----------\r\n",
    "mlmodel: Black-box-model we want to explore\r\n",
    "hyperparams: Parameter for CEM method, with following possibilites\r\n",
    "    {\r\n",
    "        \"batch_size\": int default 1,\r\n",
    "        \"kappa\": float default 0.1 Confidence parameter for the attack loss term,\r\n",
    "        \"init_learning_rate\": float default 1e-2 Initial learning rate of optimizer,\r\n",
    "        \"binary_search_steps\": int default 9 Number of steps when attacking,\r\n",
    "        \"max_iterations\": int default 100 Maximum number of iterations for finding a PN or PP,\r\n",
    "        \"initial_const\": int default 10 Initial value to scale that attack loss term,\r\n",
    "        \"beta\": float default 0.9 Regularization constant for the L1 loss term,\r\n",
    "        \"gamma\": float default 0.0 Regularization constant for the optional auto-encoder loss term,\r\n",
    "        \"mode\": str Find pertinant negatives ('PN') or pertinant positives ('PP'),\r\n",
    "        \"num_classes\": int default 2,\r\n",
    "        \"data_name\": str Name of the dataset (important for loading AE weights),\r\n",
    "        \"ae_params\":\r\n",
    "            {\r\n",
    "                \"hidden_layer\": List[int] default [20, 10, 7] Sizes of hidden layers,\r\n",
    "                \"train_ae\": bool default True,\r\n",
    "                \"epochs\": int default 5 Number of epochs to train for,\r\n",
    "            },\r\n",
    "    }\r\n",
    "\"\"\"\r\n",
    "rcm_CEM_PN = CEM(model, {\"mode\": \"PN\", \"kappa\": 0.1, \"data_name\": \"mnist\", \"ae_params\": {\"train_ae\": False, \"hidden_later\": [10, 10], \"epochs\": 5}})\r\n",
    "rcm_CEM_PP = CEM(model, {\"mode\": \"PP\", \"kappa\": 0.1, \"data_name\": \"mnist\", \"ae_params\": {\"train_ae\": False, \"hidden_later\": [10, 10], \"epochs\": 5}})\r\n",
    "# generate counterfactual examples\r\n",
    "counterfactuals_CEM_PN = rcm_CEM_PN.get_counterfactuals(factuals)\r\n",
    "counterfactuals_CEM_PP = rcm_CEM_PP.get_counterfactuals(factuals)\r\n",
    "print(\"-----COUNTERFACTUALS - CEM (PP)-----\")\r\n",
    "print(counterfactuals_CEM_PN)\r\n",
    "print(\"-----COUNTERFACTUALS - CEM (PN)-----\")\r\n",
    "print(counterfactuals_CEM_PP)\r\n",
    "print(\"--------------------------------------\")"
   ]
  }
 ]
}